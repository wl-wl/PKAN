{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from kan import *\n",
    "# from kan import *\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "import torch\n",
    "# import process as data\n",
    "import numpy as np\n",
    "import scipy.stats.stats as st\n",
    "import csv\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:31.677033Z",
     "start_time": "2024-10-02T02:03:30.234749Z"
    }
   },
   "id": "b4046f089bb163a2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "(740, 740)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取sequence.txt文件，并保存序列和标签\n",
    "sequences = []\n",
    "labels = []\n",
    "\n",
    "# 打开并读取文件内容\n",
    "with open('sequence.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 处理每两行一组的序列和标签\n",
    "for i in range(0, len(lines), 2):\n",
    "    label = lines[i].split('|')[1].strip()  # 取竖杠后面的标签\n",
    "    sequence = lines[i + 1].strip()  # 取下一行的序列\n",
    "    \n",
    "    labels.append(int(label))\n",
    "    sequences.append(sequence)\n",
    "\n",
    "len(sequences),len(labels)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:33.572886Z",
     "start_time": "2024-10-02T02:03:33.563845Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "(184, 184)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取sequence.txt文件，并保存序列和标签\n",
    "sequences_test = []\n",
    "labels_test = []\n",
    "\n",
    "# 打开并读取文件内容\n",
    "with open('sequence_test.txt', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 处理每两行一组的序列和标签\n",
    "for i in range(0, len(lines), 2):\n",
    "    label = lines[i].split('|')[1].strip()  # 取竖杠后面的标签\n",
    "    sequence = lines[i + 1].strip()  # 取下一行的序列\n",
    "    \n",
    "    labels_test.append(int(label))\n",
    "    sequences_test.append(sequence)\n",
    "\n",
    "len(sequences_test),len(labels_test)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:34.269007Z",
     "start_time": "2024-10-02T02:03:34.262596Z"
    }
   },
   "id": "9efc5720b4f3f57d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(740, 184)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def maccs_feature(file):\n",
    "    maccs_list=[]\n",
    "    with open(file,'r') as f:\n",
    "        for line in f:\n",
    "            maccs=list(map(int,line.strip().split()))\n",
    "            # print(maccs)\n",
    "            maccs_list.append(maccs)\n",
    "    return maccs_list\n",
    "train_file='maccs_fp.txt'\n",
    "test_file='maccs_fp_test.txt'\n",
    "maccs=maccs_feature(train_file)\n",
    "maccs_test=maccs_feature(test_file)\n",
    "len(maccs),len(maccs_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:35.063695Z",
     "start_time": "2024-10-02T02:03:35.046083Z"
    }
   },
   "id": "2e5c4874c3062dcc"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "(740, 184)"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_aac(protein_list):\n",
    "    amino_acids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "\n",
    "    # 初始化存储结果的列表\n",
    "    aac_list = []\n",
    "\n",
    "    for protein_sequence in protein_list:\n",
    "        # 初始化AAC特征字典\n",
    "        aac = {aa: 0 for aa in amino_acids}\n",
    "\n",
    "        # 计算每种氨基酸在序列中的频率\n",
    "        for aa in protein_sequence:\n",
    "            if aa in aac:\n",
    "                aac[aa] += 1\n",
    "\n",
    "        # 将频率转换为比例\n",
    "        sequence_length = len(protein_sequence)\n",
    "        aac = [count / sequence_length for aa, count in aac.items()]\n",
    "\n",
    "        # 将结果添加到列表中\n",
    "        aac_list.append(aac)\n",
    "\n",
    "    return aac_list\n",
    "aac = calculate_aac(sequences)\n",
    "aac_test = calculate_aac(sequences_test)\n",
    "len(aac),len(aac_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:35.655227Z",
     "start_time": "2024-10-02T02:03:35.649773Z"
    }
   },
   "id": "5c0ac15c78138b81"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740\n",
      "184\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def aaindex(filepath):\n",
    "    protein_list = []\n",
    "    csv_file_path = 'aaindex1.csv'\n",
    "    lag = 2\n",
    "    data = []\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "\n",
    "        # Skip the first cell of the first row\n",
    "        for row in csv_reader:\n",
    "            # Append the row with the first cell removed to the data list\n",
    "            # data.append([float(value) for value in row[1:]])\n",
    "            data.append(row[1:])\n",
    "    data = data[1:]\n",
    "    AAindex_list = [\n",
    "        [float(value) if value not in ['NA', ''] else None for value in row]\n",
    "        for row in data\n",
    "    ]\n",
    "    # NAKH900113 【200】，KRIW710101【146】， HUTJ700103【117】，ZIMJ680103【399】，TANS770104【368】，CEDJ970105【459】，QIAN880127【283】，LEVM760107【158】\n",
    "    # selected_indices = [200, 146, 117, 399, 368,459,283, 158]\n",
    "    # AAindex_list = [AAindex_list[i] for i in selected_indices if i < len(AAindex_list)]\n",
    "    with open(filepath, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line.startswith(\">\"):  # 忽略ID行，只保存序列行\n",
    "                protein_list.append(line)\n",
    "\n",
    "    autocorrelation = []\n",
    "    for sequence in protein_list:\n",
    "        temp = []\n",
    "        for property_values in AAindex_list:\n",
    "            # 将氨基酸序列转换为属性值序列\n",
    "            #     property_values = np.array([property_dict[aa] for aa in sequence])\n",
    "            property_values = [0 if value is None else value for value in property_values]\n",
    "            # 计算属性值的平均值\n",
    "            # print(property_values)\n",
    "            mean_value = np.mean(property_values)\n",
    "            # print(mean_value)\n",
    "            # 计算Moran自相关\n",
    "            n = len(sequence)\n",
    "            autocorr = np.sum((property_values[:-lag] - mean_value) * (property_values[lag:] - mean_value))\n",
    "            autocorr /= (n - lag)\n",
    "            temp.append(autocorr)\n",
    "\n",
    "        autocorrelation.append(temp)\n",
    "\n",
    "    v = []\n",
    "    for i in range(len(autocorrelation)):\n",
    "        vtar = autocorrelation[i]\n",
    "        vtarv = []\n",
    "        vtar7 = 0\n",
    "        vtar8 = 0\n",
    "        vtar9 = 0\n",
    "        s = pd.Series(vtar)\n",
    "        vtar3 = np.mean(vtar)  # These 4 dimensions are relevant statistical terms\n",
    "        vtar4 = st.kurtosis(vtar)\n",
    "        vtar5 = np.var(vtar)\n",
    "        vtar6 = st.skew(vtar)\n",
    "        #for p in range(len(vtar)): # These 3 dimensions are inspired by PAFIG algorithm\n",
    "        #vtar7=vtar[p]**2+vtar7\n",
    "        #if vtar[p]>va:\n",
    "        #vtar8=vtar[p]**2+vtar8\n",
    "        #else:\n",
    "        #vtar9=vtar[p]**2+vtar9\n",
    "        vcf1 = []\n",
    "        vcf2 = []\n",
    "        for j in range(len(vtar) - 1):  #Sequence-order-correlation terms\n",
    "            vcf1.append((vtar[j] - vtar[j + 1]))\n",
    "        for k in range(len(vtar) - 2):\n",
    "            vcf2.append((vtar[k] - vtar[k + 2]))\n",
    "        vtar10 = np.mean(vcf1)\n",
    "        vtar11 = np.var(vcf1)\n",
    "        vtar11A = st.kurtosis(vcf1)\n",
    "        vtar11B = st.skew(vcf1)\n",
    "        vtar12 = np.mean(vcf2)\n",
    "        vtar13 = np.var(vcf2)\n",
    "        vtar13A = st.kurtosis(vcf2)\n",
    "        vtar13B = st.skew(vcf2)\n",
    "        vtarv.append(vtar3)\n",
    "        vtarv.append(vtar4)\n",
    "        vtarv.append(vtar5)\n",
    "        vtarv.append(vtar6)\n",
    "        #vtarv.append(vtar7/len(vtar))\n",
    "        #vtarv.append(vtar8/len(vtar))\n",
    "        #vtarv.append(vtar9/len(vtar))\n",
    "        vtarv.append(vtar10)\n",
    "        vtarv.append(vtar11)\n",
    "        vtarv.append(vtar11A)\n",
    "        vtarv.append(vtar11B)\n",
    "        vtarv.append(vtar12)\n",
    "        vtarv.append(vtar13)\n",
    "        vtarv.append(vtar13A)\n",
    "        vtarv.append(vtar13B)\n",
    "        v.append(vtarv)\n",
    "    return v\n",
    "\n",
    "train_file='sequence.txt'\n",
    "test_file='sequence_test.txt'\n",
    "AC = aaindex(train_file)\n",
    "AC = np.array(AC)  # 转换为 NumPy 数组\n",
    "\n",
    "AC_test = aaindex(test_file)\n",
    "AC_test = np.array(AC_test) \n",
    "# print(v.shape,type(v))\n",
    "# 检查是否为二维数组\n",
    "if len(AC.shape) == 2:\n",
    "    # 对每一行进行 Z-score 标准化\n",
    "    AC = (AC - np.mean(AC, axis=1, keepdims=True)) / np.std(AC, axis=1, keepdims=True)\n",
    "    print(len(AC))\n",
    "else:\n",
    "    print(\"AC 不是二维数组\")\n",
    "\n",
    "if len(AC_test.shape) == 2:\n",
    "    # 对每一行进行 Z-score 标准化\n",
    "    AC_test = (AC_test - np.mean(AC_test, axis=1, keepdims=True)) / np.std(AC_test, axis=1, keepdims=True)\n",
    "    print(len(AC_test))\n",
    "else:\n",
    "    print(\"AC_test 不是二维数组\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:49.085303Z",
     "start_time": "2024-10-02T02:03:36.358279Z"
    }
   },
   "id": "1be8f637b0ec26d7"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "((740, 199), (184, 199))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings=np.concatenate((aac,maccs,AC),axis=1)\n",
    "test_encodings=np.concatenate((aac_test,maccs_test,AC_test),axis=1)\n",
    "\n",
    "# train_encodings=np.concatenate((aac,maccs),axis=1)\n",
    "# test_encodings=np.concatenate((aac_test,maccs_test),axis=1)\n",
    "# # \n",
    "# train_encodings=np.concatenate((aac,AC),axis=1)\n",
    "# test_encodings=np.concatenate((aac_test,AC_test),axis=1)\n",
    "# \n",
    "# train_encodings=np.concatenate((maccs,AC),axis=1)\n",
    "# test_encodings=np.concatenate((maccs_test,AC_test),axis=1)\n",
    "\n",
    "train_encodings.shape,test_encodings.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:55.542769Z",
     "start_time": "2024-10-02T02:03:55.531990Z"
    }
   },
   "id": "130cf99d7e92eab7"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangli/anaconda3/envs/mytest/lib/python3.7/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/wangli/anaconda3/envs/mytest/lib/python3.7/site-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/wangli/anaconda3/envs/mytest/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/wangli/anaconda3/envs/mytest/lib/python3.7/site-packages/ipykernel_launcher.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([740, 199])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X_train, y_train = shuffle(train_encodings, labels, random_state=42)\n",
    "X_test, y_test = shuffle(test_encodings, labels_test, random_state=42)\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# \n",
    "# # 初始化 StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# \n",
    "# # 对 X_train 进行标准化\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# \n",
    "# # 对 X_test 进行标准化\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train=np.array(X_train)\n",
    "X_test=np.array(X_test)\n",
    "y_train=np.array(y_train)\n",
    "y_test=np.array(y_test)\n",
    "X_train = torch.tensor(X_train)\n",
    "y_train = torch.tensor(y_train).view(-1, 1)\n",
    "X_test = torch.tensor(X_test)\n",
    "y_test = torch.tensor(y_test).view(-1, 1)\n",
    "X_train = torch.tensor(X_train ,dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test,dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test,dtype=torch.float32)\n",
    "dataset = {}\n",
    "dataset = {\n",
    "    'train_input': X_train,\n",
    "    'test_input': X_test,\n",
    "    'train_label': y_train,\n",
    "    'test_label': y_test\n",
    "}\n",
    "X_train.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:56.905633Z",
     "start_time": "2024-10-02T02:03:56.808940Z"
    }
   },
   "id": "ceb48a64d1b7f934"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([0.]), tensor([0.]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[0],y_train[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:03:57.760659Z",
     "start_time": "2024-10-02T02:03:57.751915Z"
    }
   },
   "id": "87696fc8a3b3ee9a"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint directory created: ./model\n",
      "saving model version 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 2.57e-01 | test_loss: 3.02e-01 | reg: 0.00e+00 | :  25%|▎| 1/4 [00:02<00:08,  2.81s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9162, AUC: 0.9749, SP: 0.9189, MCC: 0.8324, SN: 0.9135\n",
      "ACC: 0.8804, AUC(test): 0.9485, SP: 0.8587, MCC: 0.7616, SN: 0.9022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.91e-01 | test_loss: 3.00e-01 | reg: 0.00e+00 | :  50%|▌| 2/4 [00:05<00:05,  2.69s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9527, AUC: 0.9908, SP: 0.9541, MCC: 0.9054, SN: 0.9514\n",
      "ACC: 0.8804, AUC(test): 0.9412, SP: 0.8804, MCC: 0.7609, SN: 0.8804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.59e-01 | test_loss: 3.02e-01 | reg: 0.00e+00 | :  75%|▊| 3/4 [00:07<00:02,  2.48s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9757, AUC: 0.9975, SP: 0.9784, MCC: 0.9514, SN: 0.9730\n",
      "ACC: 0.8967, AUC(test): 0.9273, SP: 0.9239, MCC: 0.7947, SN: 0.8696\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 2\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n",
      "l: 1\n",
      "Length of self.edge_actscale: 2\n",
      "Length of self.subnode_actscale: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "| train_loss: 1.42e-01 | test_loss: 3.10e-01 | reg: 3.41e+01 | : 100%|█| 4/4 [00:12<00:00,  3.22s/it"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACC: 0.9878, AUC: 0.9993, SP: 0.9919, MCC: 0.9757, SN: 0.9838\n",
      "ACC: 0.9022, AUC(test): 0.9258, SP: 0.9348, MCC: 0.8061, SN: 0.8696\n",
      "saving model version 0.1\n",
      "0.9878378510475159 0.9021739363670349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = KAN(width=[199,5,1], grid=5, k=3, seed=42) \n",
    "#初始化完模型就可以可视化原数据\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "def train_acc():\n",
    "    pred=model(X_train)\n",
    "    with torch.no_grad():\n",
    "        pred_labels = (pred >= 0.5).float() \n",
    "        auc = roc_auc_score(y_train.cpu(), pred.cpu())\n",
    "\n",
    "    # 混淆矩阵计算\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "        # Specificity (SP)\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        # Sensitivity (SN)\n",
    "        sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_train.cpu(), pred_labels.cpu())\n",
    "    print(f\"ACC: {acc:.4f}, AUC: {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "    return torch.mean((torch.round(model(X_train)[:, 0]) == y_train[:, 0]).float())\n",
    "\n",
    "def test_acc():\n",
    "    pred=model(X_test)\n",
    "    with torch.no_grad():\n",
    "        pred_labels = (pred >= 0.5).float() \n",
    "        auc = roc_auc_score(y_test.cpu(), pred.cpu())\n",
    "\n",
    "    # 混淆矩阵计算\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "        # Specificity (SP)\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        # Sensitivity (SN)\n",
    "        sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_test.cpu(), pred_labels.cpu())\n",
    "    print(f\"ACC: {acc:.4f}, AUC(test): {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "    return torch.mean((torch.round(model(X_test)[:, 0]) == y_test[:, 0]).float())\n",
    "\n",
    "# results = model.train(dataset, opt=\"LBFGS\", steps=10, metrics=(train_acc, test_acc)) ,lamb=0.001 lamb_entropy=4.,lamb=0.1,lamb_l1=2.5,\n",
    "# lamb=0.005 train/fit   epoch30\n",
    "results = model.fit(dataset, opt=\"LBFGS\", steps=4, metrics=(train_acc, test_acc));\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-02T02:04:11.301511Z",
     "start_time": "2024-10-02T02:03:58.315323Z"
    }
   },
   "id": "adaf6279ff00895d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from kan.hypothesis import *\n",
    "# detect_separability(model, X_train, 'add')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6e14d61a9d5acb9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model.auto_swap()\n",
    "# model.plot()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "faeea07cedd7e4f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PKAN 分类 AKAN回归"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "507bf05a841a7429"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from kan.utils2 import get_derivative\n",
    "hess = get_derivative(model, X_train, y_train, derivative='hessian')\n",
    "values, vectors = torch.linalg.eigh(hess)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(values.cpu().numpy()[0], marker='o');\n",
    "plt.yscale('log')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7188dcf06abb3da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "39e41b5c96149494"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "space = {\n",
    "    # 'width': [\n",
    "    #     # hp.quniform('width1', 50, 250, 1),  # 第一层宽度，范围50到250\n",
    "    #     hp.quniform('width2', 2, 100, 1),   # 第二层宽度，范围2到100\n",
    "    #     hp.quniform('width3', 1, 10, 1)     # 第三层宽度，范围1到10\n",
    "    # ],\n",
    "    'width2': hp.quniform('width2', 2, 7, 1),\n",
    "    'width3': hp.quniform('width3', 2, 7, 1),\n",
    "    # grid值可以在一个合理的范围内调整，比如1到20\n",
    "    'grid': hp.quniform('grid', 2, 7, 1),\n",
    "    # k的取值可以在2到20之间调整\n",
    "    'k': hp.quniform('k', 2, 7, 1),\n",
    "    # seed通常不需要优化，但如果需要固定，也可以保持不变\n",
    "    # 'seed': 42  # 固定种子\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86366af3cf413239"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "\n",
    "    from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "    def train_acc():\n",
    "        pred=model(X_train)\n",
    "        with torch.no_grad():\n",
    "            pred_labels = (pred > 0.5).float()\n",
    "            auc = roc_auc_score(y_train.cpu(), pred.cpu())\n",
    "\n",
    "        # 混淆矩阵计算\n",
    "            tn, fp, fn, tp = confusion_matrix(y_train.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "            # Specificity (SP)\n",
    "            sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "            # Sensitivity (SN)\n",
    "            sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            # Matthews Correlation Coefficient (MCC)\n",
    "            mcc = matthews_corrcoef(y_train.cpu(), pred_labels.cpu())\n",
    "        print(f\"ACC: {acc:.4f}, AUC: {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "        return torch.mean((torch.round(model(X_train)[:, 0]) == y_train[:, 0]).float())\n",
    "\n",
    "    def test_acc():\n",
    "        pred=model(X_test)\n",
    "        with torch.no_grad():\n",
    "            pred_labels = (pred > 0.5).float()\n",
    "            auc = roc_auc_score(y_test.cpu(), pred.cpu())\n",
    "\n",
    "        # 混淆矩阵计算\n",
    "            tn, fp, fn, tp = confusion_matrix(y_test.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "            # Specificity (SP)\n",
    "            sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "            acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "            # Sensitivity (SN)\n",
    "            sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "            # Matthews Correlation Coefficient (MCC)\n",
    "            mcc = matthews_corrcoef(y_test.cpu(), pred_labels.cpu())\n",
    "        print(f\"ACC: {acc:.4f}, AUC(test): {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "        return torch.mean((torch.round(model(X_test)[:, 0]) == y_test[:, 0]).float())\n",
    "\n",
    "    # 使用传入的超参数初始化 KAN 模型\n",
    "    # model = KAN(width=[199,3,1], grid=int(params['grid']), k=int(params['k']), seed=params['seed'])\n",
    "    model = KAN(\n",
    "        width=[199, int(params['width2']), int(params['width3']), 1],  # 固定第一个和第三个参数，优化第二个参数\n",
    "        grid=int(params['grid']),\n",
    "        k=int(params['k']),\n",
    "        # seed=params['seed']\n",
    "    )\n",
    "    print(\"params['width2'],params['width2'],int(params['grid']),int(params['k'])\",int(params['width2']),int(params['width3']),int(params['grid']),int(params['k']))\n",
    "    # print(\"int(params['grid']),\",int(params['grid']))\n",
    "    # print(\"int(params['k'])\",int(params['k']))\n",
    "    # loss_fn=torch.nn.CrossEntropyLoss()\n",
    "    # 使用指定的优化器、训练步数和评估指标训练模型\n",
    "    # results = model.fit(dataset, opt=\"LBFGS\", steps=3, metrics=(train_acc, test_acc));\n",
    "    results = model.fit(\n",
    "        dataset, \n",
    "        opt=\"LBFGS\", \n",
    "        steps=5, \n",
    "        metrics=(train_acc, test_acc)\n",
    "        # loss_fn=loss_fn\n",
    "    )\n",
    "    weight_train = 0.3\n",
    "    weight_test = 0.7\n",
    "    # accuracy_score = weight_train * results['train_acc'][-1] + weight_test * results['test_acc'][-1]\n",
    "    # \n",
    "    train_accuracy=results['train_acc'][-1]\n",
    "    test_accuracy=results['test_acc'][-1]\n",
    "    # print('test_accuracy',test_accuracy)\n",
    "    # 获取测试集准确率（results 中第二个指标是 test_acc）\n",
    "    # test_accuracy = test_acc()\n",
    "\n",
    "    # 返回负的测试集准确率，优化时最大化准确率\n",
    "    return {\n",
    "        'loss': -test_accuracy,  # 返回负的测试准确率用于最小化\n",
    "        'test_acc': test_accuracy,  # 测试集准确率\n",
    "        'train_acc': train_accuracy,  # 训练集准确率\n",
    "        'status': 'ok'  # hyperopt 要求有 status 字段\n",
    "    } # 取负数使其最小化负准确率\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9809e03026e467a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials, space_eval\n",
    "\n",
    "# 定义 Trials 对象来跟踪训练过程\n",
    "trials = Trials()\n",
    "\n",
    "# 调用 fmin 进行优化\n",
    "best = fmin(\n",
    "    fn=objective,          # 目标函数\n",
    "    space=space,           # 搜索空间\n",
    "    algo=tpe.suggest,      # 使用TPE算法\n",
    "    max_evals=20,          # 最大评估次数\n",
    "    trials=trials          # 记录每次的评估\n",
    ")\n",
    "\n",
    "# 计算最佳参数\n",
    "best_params = space_eval(space, best)\n",
    "\n",
    "# 从 trials 中获取最好的评估结果\n",
    "best_trial = min(trials.results, key=lambda x: x['loss'])  # 'loss' 是 objective 返回值\n",
    "\n",
    "# 计算最佳准确率\n",
    "best_accuracy = -best_trial['loss']  # 取反，获得准确率\n",
    "\n",
    "print(\"最佳参数：\", best_params)\n",
    "print(\"最佳准确率：\", best_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "322206f2274941c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials, space_eval\n",
    "\n",
    "# 调用 fmin 函数进行优化\n",
    "best = fmin(\n",
    "    fn=objective,          # 目标函数\n",
    "    space=space,           # 超参数搜索空间\n",
    "    algo=tpe.suggest,      # TPE 算法用于搜索\n",
    "    max_evals=10,         # 最大评估次数\n",
    "    trials=Trials()        # 保存试验结果\n",
    ")\n",
    "# 计算最佳参数\n",
    "best_params = space_eval(space, best)\n",
    "best_accuracy = -objective(best)  # 计算最佳准确率\n",
    "\n",
    "print(\"最佳参数：\", best_params)\n",
    "print(\"最佳准确率：\", best_accuracy)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "66d7c2b94e8c7d25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6df570135a033a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3d30c7a92670c93f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# dataset['train_input']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "27eabd9a39192828"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from kan import KAN\n",
    "model = KAN(width=[199,5,1], grid=5, k=3, seed=42) \n",
    "#初始化完模型就可以可视化原数据\n",
    "# model(dataset['train_input'])\n",
    "# model.plot(beta=100)\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, matthews_corrcoef\n",
    "def train_acc():\n",
    "    pred=model(X_train)\n",
    "    with torch.no_grad():\n",
    "        pred_labels = (pred >= 0.5).float() \n",
    "        auc = roc_auc_score(y_train.cpu(), pred.cpu())\n",
    "\n",
    "    # 混淆矩阵计算\n",
    "        tn, fp, fn, tp = confusion_matrix(y_train.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "        # Specificity (SP)\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        # Sensitivity (SN)\n",
    "        sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_train.cpu(), pred_labels.cpu())\n",
    "    print(f\"ACC: {acc:.4f}, AUC: {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "    return torch.mean((torch.round(model(X_train)[:, 0]) == y_train[:, 0]).float())\n",
    "\n",
    "def test_acc():\n",
    "    pred=model(X_test)\n",
    "    with torch.no_grad():\n",
    "        pred_labels = (pred >= 0.5).float() \n",
    "        auc = roc_auc_score(y_test.cpu(), pred.cpu())\n",
    "\n",
    "    # 混淆矩阵计算\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test.cpu(), pred_labels.cpu()).ravel()\n",
    "\n",
    "        # Specificity (SP)\n",
    "        sp = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        \n",
    "        acc = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "        # Sensitivity (SN)\n",
    "        sn = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "        # Matthews Correlation Coefficient (MCC)\n",
    "        mcc = matthews_corrcoef(y_test.cpu(), pred_labels.cpu())\n",
    "    print(f\"ACC: {acc:.4f}, AUC(test): {auc:.4f}, SP: {sp:.4f}, MCC: {mcc:.4f}, SN: {sn:.4f}\")\n",
    "    return torch.mean((torch.round(model(X_test)[:, 0]) == y_test[:, 0]).float())\n",
    "\n",
    "# results = model.train(dataset, opt=\"LBFGS\", steps=10, metrics=(train_acc, test_acc)) ,lamb=0.001 lamb_entropy=4.,lamb=0.1,lamb_l1=2.5,\n",
    "# lamb=0.005 train/fit   epoch30\n",
    "# image_folder=\"/tmp/pycharm_project_763/result/\"  in_vars=[\"\"] * 200, save_fig=True,img_folder=image_folder\n",
    "img_folder='./video'\n",
    "results = model.fit(dataset, opt=\"LBFGS\", steps=5, metrics=(train_acc, test_acc), in_vars=list(range(1, 200)), out_vars=['CPP'],img_folder=img_folder)\n",
    "print(\"results['train_loss']\",results['train_loss'])\n",
    "print(\"results['test_loss']\",results['test_loss'])\n",
    "print(\"results['reg']\",results['reg'])\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])\n",
    "import moviepy\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "import os\n",
    "video_name = 'video'\n",
    "fps = 10\n",
    "# image_folder=\"/tmp/pycharm_project_763/result/\" \n",
    "# files = os.listdir(img_folder)\n",
    "# train_index = []\n",
    "# \n",
    "# for file in files:\n",
    "#     if file[0].isdigit() and file.endswith('.jpg'):\n",
    "#         train_index.append(int(file[:-4]))\n",
    "# train_index = np.sort(train_index)\n",
    "# print(train_index)\n",
    "# image_files = [img_folder + '/' + str(train_index[index]) + '.jpg' for index in train_index]\n",
    "# clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
    "# clip.write_gif(video_name + '.gif')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d20c90fcd7515fa2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model(dataset['train_input']);\n",
    "model.plot(beta=100, scale=1, in_vars=list(range(1, 200)), out_vars=['CPP'])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcf0b6d2bec554f4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "image_folder=\"/tmp/pycharm_project_763/result/\"\n",
    "results = model.fit(dataset, opt=\"LBFGS\", steps=5, metrics=(train_acc, test_acc),save_fig=True, img_folder=image_folder)\n",
    "import moviepy\n",
    "import moviepy.video.io.ImageSequenceClip\n",
    "import os\n",
    "video_name = 'video'\n",
    "fps = 10\n",
    "\n",
    "files = os.listdir(image_folder)\n",
    "train_index = []\n",
    "for file in files:\n",
    "    if file[0].isdigit() and file.endswith('.jpg'):\n",
    "        train_index.append(int(file[:-4]))\n",
    "train_index = np.sort(train_index)\n",
    "# print(train_index)\n",
    "image_files = [image_folder + '/' + str(train_index[index]) + '.jpg' for index in train_index]\n",
    "clip = moviepy.video.io.ImageSequenceClip.ImageSequenceClip(image_files, fps=fps)\n",
    "clip.write_gif(video_name + '.gif')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "77bc180a61fdec9a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 对整体的数据z-score 避免NaN  还没尝试 先修正再prune\n",
    "model = model.prune()\n",
    "model.plot(beta=100, scale=1, in_vars=list(range(1, 200)), out_vars=['CPP'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "172bb93994b6af05"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#最后一个特征有问题\n",
    "results = model.fit(dataset, opt=\"LBFGS\", steps=5, metrics=(train_acc, test_acc));\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b92480db27b580"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 修正没问题\n",
    "model = model.refine(10)\n",
    "model.plot(beta=100, scale=1, in_vars=list(range(1, 200)), out_vars=['CPP'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d78717310f84bcc4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = model.fit(dataset, opt=\"LBFGS\", steps=5, metrics=(train_acc, test_acc));\n",
    "print(results['train_acc'][-1], results['test_acc'][-1])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd4d16f4f3660579"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.plot()\n",
    "mode = \"auto\"  # \"manual\"\n",
    "\"\"\"\n",
    "假设在一个神经网络模型中，坐标 (0,36,4) 可能表示：\n",
    "\n",
    "层索引 (0): 第一层或第一个处理阶段。\n",
    "单元索引 (36): 第一层中的第 37 个单元（因为索引从 0 开始）。\n",
    "通道或特征索引 (4): 第 5 个通道或特征\n",
    "\"\"\"\n",
    "\n",
    "if mode == \"manual\":\n",
    "    # manual mode\n",
    "    model.fix_symbolic(0, 0, 0, 'sin');\n",
    "    model.fix_symbolic(0, 1, 0, 'x^2');\n",
    "    model.fix_symbolic(1, 0, 0, 'exp');\n",
    "elif mode == \"auto\":\n",
    "    # automatic mode\n",
    "    lib = ['x', 'x^2', 'x^3', 'x^4', 'exp', 'log', 'sqrt', 'tanh', 'sin', 'abs']\n",
    "    model.auto_symbolic(lib=lib)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb1d900d660389a6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "from kan.utils import ex_round\n",
    "\n",
    "ex_round(model.symbolic_formula()[0][0], 4)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "de7d53f19313d03b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lib = ['x','x^2','x^3','x^4','x^5','exp','log','sqrt','tanh','sin','tan','abs']\n",
    "model.auto_symbolic(lib=lib)\n",
    "formula = model.symbolic_formula()[0][0]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "237cc6f08ee45dcd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 读取文本文件并提取包含 'r2' 的行\n",
    "file_path = 'result.txt'  # 请根据实际文件路径修改\n",
    "\n",
    "# 提取包含 'r2' 的行\n",
    "with open(file_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "# 筛选包含 'r2' 的行\n",
    "r2_lines = [line.strip() for line in lines if 'r2' in line]\n",
    "\n",
    "# 输出结果\n",
    "for r2_line in r2_lines:\n",
    "    print(r2_line)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fb6ca457943169bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_lines = []\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if \"r2=\" in line.strip():\n",
    "        r2_value = float(line.split(\"r2=\")[1].split(\",\")[0])\n",
    "        if r2_value > 0.9:\n",
    "            filtered_lines.append(line)\n",
    "filtered_lines"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "86b7947b9f47276"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 打开文件并读取每一行\n",
    "y_list=[]\n",
    "with open('result2.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # 查找坐标开始的位置（假设格式是固定的，即'fixing('后紧跟坐标）\n",
    "        start_index = line.find('fixing(') + len('fixing(')\n",
    "        end_index = line.find(')', start_index)\n",
    "        # 提取坐标字符串\n",
    "        coordinates = line[start_index:end_index]\n",
    "        # 分割坐标字符串来获取各个坐标\n",
    "        x, y, z = coordinates.split(',')\n",
    "        y_list.append(int(y))\n",
    "        # 打印第二个坐标值（y坐标）\n",
    "        # print(y.strip())\n",
    "# y_list\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "96de08ce03a479bf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_prune=[[row[i] for i in y_list] for row in X]\n",
    "y_prune=[[row[i] for i in y_list] for row in y]\n",
    "len(X_prune),len(X_prune[0])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e43007df5363c509"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns  # 定义部分依赖函数\n",
    "\n",
    "\n",
    "def partial_dependence(feature_idx, feature_values, formula, dataset):\n",
    "    predictions = []\n",
    "    for value in feature_values:\n",
    "        subs_dict = {\n",
    "            'x_{}'.format(i + 1): value if i == feature_idx else dataset['test_input'][0, i]\n",
    "            for i in range(dataset['test_input'].shape[1])\n",
    "        }\n",
    "        predict = float(formula.subs(subs_dict))\n",
    "        predictions.append(predict)\n",
    "    return np.array(predictions)\n",
    "\n",
    "\n",
    "# dataset 是你的数据集，formula 是你的模型公式\n",
    "# 选择要绘制部分依赖图的特征索引和范围\n",
    "feature_index_pdp = 0  # 假设选择第一个特征进行 PDP\n",
    "feature_values_pdp = np.linspace(min(dataset['test_input'][:, feature_index_pdp]), max(dataset['test_input'][:, feature_index_pdp]), num=50)\n",
    "# 计算部分依赖\n",
    "predictions_pdp = partial_dependence(feature_index_pdp, feature_values_pdp, formula, dataset)\n",
    "# 绘制部分依赖图（PDP）# 设置Seaborn样式\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(feature_values_pdp, predictions_pdp, marker='o', linestyle='-', color='b', linewidth=2, markersize=6)  # 添加标题和轴标签，并设置字体大小\n",
    "plt.title('Partial Dependence Plot (PDP) for Feature {}'.format(feature_index_pdp), fontsize=16)\n",
    "plt.xlabel('Feature Value', fontsize=14)\n",
    "plt.ylabel('Average Prediction', fontsize=14)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# 添加背景和框架\n",
    "sns.despine(left=True, bottom=True)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d80ba28e9f5dbf12"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns  #\n",
    "def ice(feature_idx, feature_values, formula, dataset):\n",
    "    ice_predictions = []\n",
    "    for i in range(dataset['test_input'].shape[0]):\n",
    "        individual_predictions = []\n",
    "        for value in feature_values:\n",
    "            subs_dict = {\n",
    "                'x_{}'.format(j + 1): value if j == feature_idx else dataset['test_input'][i, j]\n",
    "                for j in range(dataset['test_input'].shape[1])\n",
    "            }\n",
    "            predict = float(formula.subs(subs_dict))\n",
    "            individual_predictions.append(predict)\n",
    "            ice_predictions.append(individual_predictions)\n",
    "    return np.array(ice_predictions)\n",
    "\n",
    "\n",
    "# 选择要绘制个体条件期望（ICE）的特征索引和范围\n",
    "feature_index_ice = 0  # 假设选择第一个特征进行 ICE\n",
    "feature_values_ice = np.linspace(min(dataset['test_input'][:, feature_index_ice]), max(dataset['test_input'][:, feature_index_ice]), num=10)\n",
    "\n",
    "# 计算个体条件期望（ICE）\n",
    "ice_predictions = ice(feature_index_ice, feature_values_ice, formula, dataset)\n",
    "# 绘制个体条件期望（ICE）图\n",
    "# \n",
    "plt.figure(figsize=(15, 5))\n",
    "for i in range(len(ice_predictions)):\n",
    "    plt.plot(feature_values_ice, ice_predictions[i], color='grey', alpha=0.2)\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Prediction')\n",
    "    plt.title('Individual Conditional Expectation (ICE) Plot for Feature {}'.format(feature_index_ice))\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "from mpl_toolkits.mplot3d import Axes3D  # 定义二维部分依赖函数\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74cb77dac60b5c57"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns  #\n",
    "def partial_dependence_2d(feature_idx1, feature_idx2, feature_values1, feature_values2, formula, dataset):\n",
    "    predictions = np.zeros((len(feature_values1), len(feature_values2)))\n",
    "    for i, value1 in enumerate(feature_values1):\n",
    "        for j, value2 in enumerate(feature_values2):\n",
    "            subs_dict = {\n",
    "                'x_{}'.format(k + 1): value1 if k == feature_idx1 else value2 if k == feature_idx2 else dataset['test_input'][0, k]\n",
    "                for k in range(dataset['test_input'].shape[1])\n",
    "            }\n",
    "            predict = float(formula.subs(subs_dict))\n",
    "            predictions[i, j] = predict\n",
    "    return predictions  # 选择要绘制二维部分依赖图的特征索引和范围\n",
    "\n",
    "\n",
    "feature_index1 = 0  # 第一个特征索引\n",
    "feature_index2 = 1  # 第二个特征索引\n",
    "feature_values1 = np.linspace(min(dataset['test_input'][:, feature_index1]), max(dataset['test_input'][:, feature_index1]), num=10)\n",
    "feature_values2 = np.linspace(min(dataset['test_input'][:, feature_index2]), max(dataset['test_input'][:, feature_index2]), num=10)  # 计算二维部分依赖\n",
    "predictions_2d_pdp = partial_dependence_2d(feature_index1, feature_index2, feature_values1, feature_values2, formula, dataset)  # 绘制二维部分依赖图（2D PDP）\n",
    "sns.set(style=\"whitegrid\")\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')  # 创建网格\n",
    "X, Y = np.meshgrid(feature_values1, feature_values2)\n",
    "surf = ax.plot_surface(X, Y, predictions_2d_pdp, cmap='viridis', edgecolor='k', linewidth=0.5, alpha=0.9)\n",
    "fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5, pad=0.1)\n",
    "ax.set_xlabel('Feature {}'.format(feature_index1), fontsize=14, labelpad=10)\n",
    "ax.set_ylabel('Feature {}'.format(feature_index2), fontsize=14, labelpad=10)\n",
    "ax.set_zlabel('Average Prediction', fontsize=14, labelpad=10)\n",
    "ax.set_title('2D Partial Dependence Plot (PDP) for Features {} and {}'.format(feature_index1, feature_index2), fontsize=16, pad=20)\n",
    "ax.tick_params(axis='both', which='major', labelsize=12)  # 设置视角\n",
    "ax.view_init(elev=60, azim=120)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "31ef23824d0fbfd8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_tree(model, X_train)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cf5f1b43c8b2801"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix, roc_curve\n",
    "from sklearn.datasets import make_classification\n",
    "#以下代码是一个3层的\n",
    "# 生成一个示例数据集\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义MLP模型\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,10),(20,20),(30,), (40,), (50,), (60,), (70,), (80,),(90,),(100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "# 使用GridSearchCV进行网格搜索\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 最佳参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "train_preds = grid_search.predict(X_train)\n",
    "test_preds = grid_search.predict(X_test)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # Sensitivity (SP)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # Specificity (SE)\n",
    "    specificity = tn / (tn + fp)\n",
    "    # MCC\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return sensitivity, specificity, mcc, accuracy\n",
    "\n",
    "# 训练集评估\n",
    "train_sensitivity, train_specificity, train_mcc, train_accuracy = calculate_metrics(y_train, train_preds)\n",
    "print(f\"Training Sensitivity (SP): {train_sensitivity:.4f}\")\n",
    "print(f\"Training Specificity (SE): {train_specificity:.4f}\")\n",
    "print(f\"Training MCC: {train_mcc:.4f}\")\n",
    "print(f\"Training Accuracy (ACC): {train_accuracy:.4f}\")\n",
    "\n",
    "# 测试集评估\n",
    "test_sensitivity, test_specificity, test_mcc, test_accuracy = calculate_metrics(y_test, test_preds)\n",
    "print(f\"Test Sensitivity (SP): {test_sensitivity:.4f}\")\n",
    "print(f\"Test Specificity (SE): {test_specificity:.4f}\")\n",
    "print(f\"Test MCC: {test_mcc:.4f}\")\n",
    "print(f\"Test Accuracy (ACC): {test_accuracy:.4f}\")\n",
    "\n",
    "# AUC计算\n",
    "test_probs = grid_search.predict_proba(X_test)[:, 1]\n",
    "train_probs = grid_search.predict_proba(X_train)[:, 1]\n",
    "\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "print(f\"Training AUC: {train_auc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52324e5803e51170"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, matthews_corrcoef, confusion_matrix, roc_curve\n",
    "from sklearn.datasets import make_classification\n",
    "#以下代码是一个3层的\n",
    "# 生成一个示例数据集\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# 定义MLP模型\n",
    "mlp = MLPClassifier(max_iter=1000)\n",
    "\n",
    "# 定义参数网格\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(10,10),(20,20),(30,30), (40,40), (50,50), (60,60), (70,70), (80,80),(90,90),(100,100)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['adam', 'sgd'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "}\n",
    "\n",
    "# 使用GridSearchCV进行网格搜索\n",
    "grid_search = GridSearchCV(mlp, param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# 最佳参数\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "train_preds = grid_search.predict(X_train)\n",
    "test_preds = grid_search.predict(X_test)\n",
    "\n",
    "# 计算混淆矩阵\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    # Sensitivity (SP)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    # Specificity (SE)\n",
    "    specificity = tn / (tn + fp)\n",
    "    # MCC\n",
    "    mcc = matthews_corrcoef(y_true, y_pred)\n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return sensitivity, specificity, mcc, accuracy\n",
    "\n",
    "# 训练集评估\n",
    "train_sensitivity, train_specificity, train_mcc, train_accuracy = calculate_metrics(y_train, train_preds)\n",
    "print(f\"Training Sensitivity (SP): {train_sensitivity:.4f}\")\n",
    "print(f\"Training Specificity (SE): {train_specificity:.4f}\")\n",
    "print(f\"Training MCC: {train_mcc:.4f}\")\n",
    "print(f\"Training Accuracy (ACC): {train_accuracy:.4f}\")\n",
    "\n",
    "# 测试集评估\n",
    "test_sensitivity, test_specificity, test_mcc, test_accuracy = calculate_metrics(y_test, test_preds)\n",
    "print(f\"Test Sensitivity (SP): {test_sensitivity:.4f}\")\n",
    "print(f\"Test Specificity (SE): {test_specificity:.4f}\")\n",
    "print(f\"Test MCC: {test_mcc:.4f}\")\n",
    "print(f\"Test Accuracy (ACC): {test_accuracy:.4f}\")\n",
    "\n",
    "# AUC计算\n",
    "test_probs = grid_search.predict_proba(X_test)[:, 1]\n",
    "train_probs = grid_search.predict_proba(X_train)[:, 1]\n",
    "\n",
    "train_auc = roc_auc_score(y_train, train_probs)\n",
    "test_auc = roc_auc_score(y_test, test_probs)\n",
    "print(f\"Training AUC: {train_auc:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca817b305db3cb16"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2f9638c5d852064b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "723e632744f6791"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "protein_list = []\n",
    "with open('/tmp/pycharm_project_763/model/sequence_test.txt', 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if not line.startswith(\">\"):  # 忽略ID行，只保存序列行\n",
    "                protein_list.append(line)\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "IPythonConsole.ipython_useSVG = True\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem, MACCSkeys\n",
    "\n",
    "with open('/tmp/pycharm_project_763/model/maccs_fp_test.txt', 'w') as f:\n",
    "    for seq in protein_list:\n",
    "        peptide = Chem.MolToSmiles(Chem.MolFromFASTA(seq))\n",
    "        peptide_smiles = Chem.MolFromFASTA(seq)\n",
    "        if peptide_smiles is None:\n",
    "            raise ValueError(\"SMILES字符串无效，无法转换为Mol对象\")\n",
    "        print(type(peptide_smiles))\n",
    "        maccs_fp = MACCSkeys.GenMACCSKeys(peptide_smiles)\n",
    "        # print(' '.join(list(maccs_fp)))\n",
    "        print(' '.join(map(str, list(maccs_fp))))\n",
    "        maccs_fp = ' '.join(map(str, list(maccs_fp)))\n",
    "        # f.write(peptide+'\\t'+str(maccs_fp)+'\\n')\n",
    "        f.write(maccs_fp + '\\n')\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "315f3780c0d10e4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
